{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2deeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Twikit es un modulo de python desarrollado por personas independientes. Este modulo lo que hace es 'simular' lo que haría una persona para adquirir los datos de X (anteriormente llamado twitter) SIN USAR LA API: toma, por ejemplo, los datos personales para inicio de sesión y, valga la redundancia, con ello abre una sesión de X y luego ahí sí puede capturar los tweets o datos según se le indique. También se puede hacer uso de las cookies para no tener que ingresar los datos de inicio de sesión. EXISTE RIESGO DE SUSPENSIÓN DE CUENTA EN CASO TAL DE INCUMPLIR CON LAS NORMAS DE LA RED SOCIAL, así que toca usarlo sabiamente, por ejemplo: scrappeando los tweets en intervalos de tiempo lejanos (yo he utilizado cada 15 o 30 minutos durante 8-9 horas y sin sobrepasarme de los 300 tweets al dia que X permite visualizar para un usuario nuevo y no verificado)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import asyncio\n",
    "from twikit import Client  # No need to write \"from twikit.twikit_async import Client\" \n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "#Se necesita usar la librería twikit para poder hacer el scrapping de los tweets. Para instalar, basta con usar: pip install twikit.\n",
    "#La versión de python que funciona para usar esta librería es la 3.11 o posteriores.\n",
    "\n",
    "\"\"\"Twikit es un modulo de python desarrollado por personas independientes. Este modulo lo que hace es 'simular' lo que haría una persona para adquirir los datos de X (anteriormente llamado twitter) SIN USAR LA API: toma, por ejemplo, los datos personales para inicio de sesión y, valga la redundancia, con ello abre una sesión de X y luego ahí sí puede capturar los tweets o datos según se le indique. También se puede hacer uso de las cookies para no tener que ingresar los datos de inicio de sesión. EXISTE RIESGO DE SUSPENSIÓN DE CUENTA EN CASO TAL DE INCUMPLIR CON LAS NORMAS DE LA RED SOCIAL, así que toca usarlo sabiamente, por ejemplo: scrappeando los tweets en intervalos de tiempo lejanos (yo he utilizado cada 15 o 30 minutos durante 8-9 horas y sin sobrepasarme de los 300 tweets al dia que X permite visualizar para un usuario nuevo y no verificado)\"\"\"\n",
    "\n",
    "\n",
    "#NOTA: El método \"created_at_datetime\" está en utc ± 0, por lo tanto, toca restar 5 horas a la fecha por lo de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c197b60-127f-4024-a04e-ede51e0d286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos de inicio de sesión de X (Todo va en string)\n",
    "USERNAME = \"\"\n",
    "EMAIL = \"\"\n",
    "PASSWORD = \"\"\n",
    "\n",
    "#Se debe inicializar el cliente\n",
    "\n",
    "client = Client('es-MX')\n",
    "\n",
    "await client.login(\n",
    "    auth_info_1=USERNAME ,\n",
    "    auth_info_2=EMAIL,\n",
    "    password=PASSWORD)\n",
    "\n",
    "client.save_cookies('cookies.json') #Guarda las cookies para que no tenga que volver a iniciar sesión en la cuenta X y así evitar ser suspendida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8802fd32-3f15-4ec2-a4be-5fb1e42ccd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main():\n",
    "    client.load_cookies('cookies.json') #Carga las cookies anteriormente guardadas. \n",
    "\n",
    "    print ('Ejecutando Script...') #Un print controlador: solo sirve para saber si se está ejecutando la función. Es opcional.\n",
    "    tweets = await client.search_tweet('#Alberto OR #Monterrey OR #NuevoLeon OR #NuevoLeón OR #inundaciones OR Monterrey','Latest', count = 20) #Variable que busca los tweets con el metodo \".search_tweet\" de la librería. El parametro \"count\" es la cantidad de tweets a recolectar. El límite de tweets es de 20. Solo se pueden buscar los tweets que están en la sección \"Top\", \"Latest\" o \"Media\". Se pueden concatenar terminos con el operador OR dentro del string. until:2024-6-26\n",
    "    more_tweets = await tweets.next() #Variable que ayuda a buscar más tweets aparte de los que se buscan con el método \".search_tweets\". Se pueden concatenar, así como lo hice en la linea. Sin embargo, también tiene el limite de 20 tweets y maximo 50 tweets por cada 15 minutos. Si se excede ese limite, el metodo no funciona y por lo tanto el bloque de codigo ya no sirve. Toca esperar a que se actualice.\n",
    "    \n",
    "    #for tweet in tweets:\n",
    "    #    print(\"-\",\n",
    "    #        tweet.user.name,\n",
    "    #        tweet.text,\n",
    "    #        tweet.created_at_datetime - timedelta(hours=5), #Se resta 5 horas para cuadrar la zona horaria\n",
    "    #    )\n",
    "\n",
    "    final_tweets = [] #Lista donde guardaremos los tweets con sus datos de usuario y fecha\n",
    "    for tweet in tweets:\n",
    "        data = [tweet.user.name, tweet.text, (tweet.created_at_datetime - timedelta(hours=5)).strftime(\"%d/%m/%Y %H:%M:%S\")] #Lista de datos: usuario, texto del tweet y fecha en formato \"%d/%m/%Y %H:%M:%S\". El \"- timedelta(hours=5)\" es por la nota del inicio del código.\n",
    "        final_tweets.append(data) #Se añade los datos a la lista\n",
    "    \n",
    "    #Se hace lo mismo que anteriormente hicimos, pero en este caso, para la variable que nos recolecta más tweets\n",
    "    for tweet in more_tweets:\n",
    "        data = [tweet.user.name, tweet.text, (tweet.created_at_datetime - timedelta(hours=5)).strftime(\"%d/%m/%Y %H:%M:%S\")]\n",
    "        final_tweets.append(data) \n",
    "\n",
    "    #Cargamos el excel\n",
    "    df_excel = pd.read_excel(\"Base.xlsx\")\n",
    "\n",
    "    #Guardamos los ultimos tweets de arriba\n",
    "    data = pd.DataFrame(final_tweets, columns = [\"User\", \"Text\", \"Date\"])\n",
    "\n",
    "    #Concatenamos\n",
    "    Excel = pd.concat([df_excel, data])\n",
    "\n",
    "    #Eliminamos duplicados \n",
    "    df_sin_duplicados = Excel.drop_duplicates()\n",
    "\n",
    "    #Creamos el archivo\n",
    "    df_sin_duplicados.to_excel(\"Base.xlsx\", index = False) #Guardo en formato excel para luego, desde el propio excel, exportarlo. Me genera \"menos inconveniente\" hacerlo así. \n",
    "    time.sleep(600) #Linea que sirve para congelar el bloque de codigo que ayuda a scrappear los tweets. Esto se hace para recolectar los tweets cada t segundos y así evitar que: 1) el bucle genere conflicto a la hora de ejecutar la función que añade al excel los datos y 2) Nos suspendan la cuenta. #Recomiendo que se recolecte cada 15 o 30 minutos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb87017-6057-430d-a2b4-108d77ec949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El contador va en: 0\n",
      "Ejecutando Script...\n",
      "El contador va en: 1\n",
      "Ejecutando Script...\n",
      "El contador va en: 2\n",
      "Ejecutando Script...\n",
      "El contador va en: 3\n",
      "Ejecutando Script...\n",
      "El contador va en: 4\n",
      "Ejecutando Script...\n"
     ]
    }
   ],
   "source": [
    "#Contador auxiliar que nos ayuda a interrumpir el bucle infinito \n",
    "contador = 0\n",
    "\n",
    "\n",
    "#Bucle para no tener que manualmente estar ejecutando el script\n",
    "while True: \n",
    "    print(\"El contador va en:\", contador) #Print que ayuda a saber en qué iteración va y también nos sirve para hacer calculo mental de cuantos tweets aproximadamente se pudieron haber scrappeado hasta el momento\n",
    "    await main() #Se ejecuta la función para recolectar los tweets y guardarlos en el excel\n",
    "    contador += 1\n",
    "    if contador == 5: #Numero de veces que queramos hacer la recolección de tweets. Por ejemplo, si quiero recolectar 10 veces, entonces coloco 10. Si todo sale bien, eso serían entonces 20 tweets x 10 = 200 tweets cada t segundos que haya colocado arriba en el \"time.sleep()\".\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46243b42-c86b-40fb-818f-5799a4a1fe29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
